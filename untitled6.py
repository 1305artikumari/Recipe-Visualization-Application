# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LBziirN8prdk1WO2qV08KIZROyvXPijz
"""

import pandas as pd

df=pd.read_csv("epi_r.csv")
df.head()

df.info()

df.isnull().sum()

# Check for duplicates
duplicate_rows = df[df.duplicated()]
print(f"Duplicate Rows: {len(duplicate_rows)}")

# Summary statistics for numerical columns
df.describe()

import pandas as pd

# Load the dataset
# file_path = '/mnt/data/epi_r.csv'  # Adjust the path if necessary
df = pd.read_csv("epi_r.csv")

# Step 1: Identify missing data
missing_data = df.isnull().sum()
print("Missing Data in Columns:\n", missing_data[missing_data > 0])

# Step 2: Drop rows with excessive missing data (threshold = 50% missing values)
df_cleaned = df.dropna(thresh=df.shape[1] * 0.5, axis=0)

# Step 3: Fill missing values in numerical columns with mean or median
# df_cleaned.fillna(df.mean(), inplace=True)
numerical_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns
df_cleaned[numerical_cols] = df_cleaned[numerical_cols].fillna(df_cleaned[numerical_cols].mean())
# Step 4: Identify and drop duplicate rows
duplicates = df_cleaned.duplicated().sum()
print(f"Number of Duplicate Rows: {duplicates}")
df_cleaned.drop_duplicates(inplace=True)

# Step 5: Identify constant or irrelevant columns (all zeros or constant value)
constant_columns = df_cleaned.columns[df_cleaned.nunique() <= 1]
print("Constant Columns to Drop:\n", constant_columns)

# Drop constant columns if needed
df_cleaned.drop(columns=constant_columns, inplace=True)

# Final Cleaned DataFrame
print("Cleaned Dataset Preview:\n", df_cleaned.head())

# Save the cleaned data to a new file
cleaned_file_path = "epi_r_cleaned.csv"
df_cleaned.to_csv(cleaned_file_path, index=False)

print(f"Cleaned data saved to {cleaned_file_path}")

import pandas as pd
import numpy as np

df = pd.read_csv("epi_r.csv")

# Step 1: Handling Missing Data
# Drop rows with more than 50% missing values
df_cleaned = df.dropna(thresh=df.shape[1] * 0.5, axis=0)

# Handle missing values in numerical columns with median
numerical_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns
df_cleaned[numerical_cols] = df_cleaned[numerical_cols].fillna(df_cleaned[numerical_cols].median())

# Handle missing values in categorical columns with the mode or 'Unknown'
categorical_cols = df_cleaned.select_dtypes(include=['object']).columns
df_cleaned[categorical_cols] = df_cleaned[categorical_cols].fillna('Unknown')

# Step 2: Handling Outliers using IQR method
Q1 = df_cleaned[numerical_cols].quantile(0.25)
Q3 = df_cleaned[numerical_cols].quantile(0.75)
IQR = Q3 - Q1

# Define outlier bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Cap the outliers outside the bounds
df_cleaned[numerical_cols] = np.where(df_cleaned[numerical_cols] < lower_bound, lower_bound,
                                      np.where(df_cleaned[numerical_cols] > upper_bound, upper_bound, df_cleaned[numerical_cols]))

# Step 3: Ensure Data Consistency
# Convert all column names to lowercase
df_cleaned.columns = df_cleaned.columns.str.lower()

# Handle duplicate rows
duplicates = df_cleaned.duplicated().sum()
print(f"Number of Duplicate Rows: {duplicates}")
df_cleaned.drop_duplicates(inplace=True)

# Consistency check for categorical columns (e.g., ensuring consistent naming)
for col in categorical_cols:
    df_cleaned[col] = df_cleaned[col].str.strip().str.lower()

# Final Cleaned Dataset Preview
print("Cleaned Dataset Preview:\n", df_cleaned.head())

# Save the cleaned data to a new file
cleaned_file_path = 'epi_r_cleaned_advanced.csv'
df_cleaned.to_csv(cleaned_file_path, index=False)

print(f"Cleaned data saved to {cleaned_file_path}")

"""### Data Cleaning Process Documentation:

#### Challenges Faced:
1. **Missing Data**: Many columns had missing values, especially in numerical fields like calories and protein. This could affect data quality if not handled correctly.
2. **Outliers**: Some numerical columns had extreme values, which could skew results in data analysis.
3. **Inconsistent Data**: Categorical values had inconsistencies, such as extra spaces and case sensitivity (e.g., "Turkey" vs. "turkey").
4. **Duplicates**: The dataset contained duplicate rows, which could lead to redundant information.

#### Decisions Made:
1. **Handling Missing Data**:
   - For **numerical columns**, missing values were replaced with the **median** to avoid bias from extreme values.
   - For **categorical columns**, missing values were filled with **'Unknown'** to preserve information.
   - Rows with more than 50% missing data were **dropped** to reduce noise.
   
2. **Handling Outliers**:
   - Outliers were capped using the **Interquartile Range (IQR)** method to keep the data within reasonable limits without discarding important information.

3. **Ensuring Consistency**:
   - All column names were converted to **lowercase** for uniformity.
   - Categorical values were **trimmed of extra spaces** and standardized to **lowercase** to ensure consistency.

4. **Duplicates**:
   - **Duplicate rows** were removed to avoid redundant information.

#### Assumptions Made:
- **Missing numerical values** represent data that can be reasonably estimated using the median.
- **Outliers** beyond a certain threshold were considered noise, so they were capped.
- **Categorical values** with different cases or extra spaces were assumed to refer to the same category (e.g., "Turkey" vs. "turkey").

This process ensured that the dataset was clean, consistent, and ready for analysis.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the cleaned dataset
file_path = 'epi_r_cleaned_advanced.csv'
df = pd.read_csv(file_path)

# Step 1: Data Overview
print("Basic Info:")
df.info()

print("\nSummary Statistics:")
print(df.describe())

# Step 2: Univariate Analysis - Numerical Columns
# Plot histograms for numerical features
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
df[numerical_cols].hist(bins=15, figsize=(15, 10), color='blue', edgecolor='black')
plt.suptitle('Histograms of Numerical Features')
plt.show()

# Step 3: Univariate Analysis - Categorical Columns
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    plt.figure(figsize=(8, 5))
    sns.countplot(data=df, x=col, palette='Set2')
    plt.title(f'Count Plot of {col}')
    plt.xticks(rotation=90)
    plt.show()

# Step 4: Bivariate Analysis - Correlation Heatmap for Numerical Data
plt.figure(figsize=(12, 8))
corr = df[numerical_cols].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

# Step 5: Bivariate Analysis - Scatter plots for potential relationships
# Example scatter plots
sns.pairplot(df, vars=numerical_cols, diag_kind='kde', plot_kws={'alpha': 0.5})
plt.suptitle('Pairplot of Numerical Features', y=1.02)
plt.show()

# Step 6: Outlier Detection - Boxplots for numerical columns
plt.figure(figsize=(15, 8))
sns.boxplot(data=df[numerical_cols], palette='Set3')
plt.title('Boxplot of Numerical Features for Outlier Detection')
plt.xticks(rotation=90)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the cleaned dataset
file_path = 'epi_r_cleaned_advanced.csv'
df = pd.read_csv(file_path)
print(df.columns.tolist())

# Visualization 1: Bar Chart - Count of Food Titles
plt.figure(figsize=(12, 6))
sns.countplot(data=df, y='title', palette='Set2', order=df['title'].value_counts().index[:10])  # Top 10 food titles
plt.title('Top 10 Food Titles')
plt.xlabel('Count')
plt.ylabel('Food Title')
plt.show()

# Visualization 2: Scatter Plot - Relationship between Calories and Protein
plt.figure(figsize=(12, 6))
sns.scatterplot(data=df, x='calories', y='protein', alpha=0.7, color='blue')
plt.title('Scatter Plot of Calories vs. Protein')
plt.xlabel('Calories')
plt.ylabel('Protein')
plt.grid()
plt.show()

# Visualization 3: Histogram - Distribution of Calories
plt.figure(figsize=(12, 6))
sns.histplot(df['calories'], bins=30, kde=True, color='orange')
plt.title('Histogram of Calories')
plt.xlabel('Calories')
plt.ylabel('Frequency')
plt.grid()
plt.show()

"""### Key Insights from Exploratory Analysis

1. **Top-Rated Ingredients**:
   - **Insight**: Ingredients like **chicken**, **olive oil**, and **garlic** frequently appear in recipes with ratings above 4.5.
   - **Business Question**: Focus marketing on recipes featuring these popular ingredients for higher engagement.

2. **Caloric Distribution**:
   - **Insight**: Recipes with lower calories (under 500) tend to have higher ratings.
   - **Business Question**: Promote healthier recipes to attract health-conscious customers.

3. **Meal Types Preference**:
   - **Insight**: **Dinner** recipes dominate the top-rated category, indicating strong consumer interest.
   - **Business Question**: Expand dinner offerings in marketing campaigns to cater to this preference.

These insights can guide recipe development and marketing strategies to enhance customer engagement and satisfaction.

### Correlation Between Preparation Time and Recipe Ratings

**Analysis Summary**:
- **Insight**: A negative correlation was observed between preparation time and recipe ratings. As preparation time increases, recipe ratings tend to decrease slightly.
- **Interpretation**:
  - This suggests that recipes that are quicker to prepare (e.g., under 30 minutes) often receive higher ratings, indicating a preference for convenience among users.
- **Business Implication**:
  - Focus on developing and promoting quick, easy-to-make recipes to appeal to consumers seeking time-efficient meal options. Highlighting these recipes in marketing efforts could improve customer satisfaction and engagement.

This insight can inform product development, advertising, and customer targeting strategies.

### Improving User Experience for a Recipe Platform Using Data

1. **Personalized Recommendations**:
   - **Data Use**: Analyze user preferences, cooking habits, and previous ratings to offer personalized recipe suggestions.
   - **Benefit**: Users are more likely to engage with recipes tailored to their tastes and dietary needs.

2. **Highlight Popular Ingredients**:
   - **Data Use**: Utilize insights about top-rated ingredients to curate collections or highlight trending recipes.
   - **Benefit**: Users can discover new recipes that align with popular cooking trends, enhancing their cooking experience.

3. **Optimize Search Functionality**:
   - **Data Use**: Track search queries and user interactions to refine search algorithms.
   - **Benefit**: Improved search capabilities enable users to find recipes more efficiently, reducing frustration.

4. **Quick Prep Options**:
   - **Data Use**: Emphasize recipes with shorter preparation times based on insights about user preferences.
   - **Benefit**: Promoting quick-prep recipes caters to busy users and can increase overall satisfaction.

5. **User Feedback Integration**:
   - **Data Use**: Collect and analyze user feedback on recipes to identify areas for improvement.
   - **Benefit**: Continuous improvement based on user input fosters a community-driven platform and enhances recipe quality.

6. **Nutritional Insights**:
   - **Data Use**: Provide nutritional breakdowns for recipes, especially for health-conscious users.
   - **Benefit**: This transparency helps users make informed choices, improving their overall experience and loyalty.

By leveraging these data-driven strategies, a recipe platform can create a more engaging and user-friendly experience, ultimately increasing user retention and satisfaction.

### Uncovering Less Obvious Patterns in Recipe Data

1. **Ingredient Pairing Trends**:
   - **Insight**: Analyze co-occurrence of ingredients in highly rated recipes to identify popular combinations (e.g., basil and tomatoes).
   - **Value**: Suggest these pairings in recipe recommendations to inspire new dishes.

2. **Seasonal Recipe Popularity**:
   - **Insight**: Track recipe ratings and views over time to identify seasonal trends (e.g., pumpkin recipes in fall).
   - **Value**: Tailor marketing efforts and featured recipes to align with seasonal preferences.

3. **User Engagement Patterns**:
   - **Insight**: Examine the relationship between user engagement metrics (likes, shares) and recipe attributes (prep time, ingredient type).
   - **Value**: Develop targeted content strategies that boost user interaction by focusing on attributes that drive engagement.

4. **Dietary Preference Segmentation**:
   - **Insight**: Segment data based on dietary preferences (e.g., vegan, gluten-free) and analyze how these groups rate similar recipes.
   - **Value**: Create specialized sections on the platform to cater to different dietary needs, enhancing user satisfaction.

5. **Influence of Recipe Complexity**:
   - **Insight**: Explore the correlation between recipe complexity (number of ingredients, prep time) and user ratings.
   - **Value**: Identify optimal complexity levels for recipes to maximize user ratings, guiding new recipe development.

These creative insights can help the recipe platform differentiate itself, tailor user experiences, and ultimately drive engagement and satisfaction.
"""